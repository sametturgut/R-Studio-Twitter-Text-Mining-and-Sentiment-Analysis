library(ROAuth)
library(twitteR)
library(tm)
library(wordcloud)
library(ggplot2)
library(Rcolor)
library(ROAuth)
library(twitteR)
library(tm)
library(dplyr)
library(RCurl)
library(SnowballC)
library(openssl)
library(wordcloud)
library(ggplot2)
library(RColorBrewer)
library(stringr)
library(tidytext)
library(readr)
library(stringi)
library(xlsx)
options(httr_oauth_cache=T)
APIkey <- "X4Ldvspy9rJPbQQvIgKZUJTD3"
APIsecret <- "e9RTNSQnOZN7G7cO0BJhQ3dFHNuV6cEF879ihjpcIeHStzEGXb"
access_token <- "802721334-yCrIMy3wIWFZyqmG8Qv8xSOQzgLlAp1qptXd4Sxa"
access_secret <- "DW3h24r4jHhASzSqwOvMSmqriFlCRoACWmKeLvlp4AFxQ"
setup_twitter_oauth(APIkey, APIsecret, access_token, access_secret)
steam <- searchTwitteR("steam", n=5000,  lang = "tr")
steam.df <- twListToDF(steam)
length(steam)
tweet_clean <- steam.df
tweet_clean$text <- stri_enc_toutf8(tweet_clean$text)
tweet_clean$text <- ifelse(str_sub(tweet_clean$text,1,2) == "rt",
substring(tweet_clean$text,3),
tweet_clean$text)
#URL linklerinin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "http[^[:space:]]*", "")
#Hashtag "#" ve "@" iÅaretlerinin kaldÄ±rÄ±lmasÄ±
tweet_clean$text <- str_replace_all(tweet_clean$text, "#\\S+", "")
tweet_clean$text <- str_replace_all(tweet_clean$text, "@\\S+", "")
#Noktalama iÅaretlerinin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[[:punct:][:blank:]]+", " ")
#TÃ¼m harflerin kÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅtÃ¼rÃ¼lmesi
tweet_clean$text  <- str_to_lower(tweet_clean$text, "tr")
#RakamlarÄ±n temizlenmesi
tweet_clean$text <- removeNumbers(tweet_clean$text)
#ASCII formatÄ±na uymayan karakterlerin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[<].*[>]", "")
tweet_clean$text <- gsub("\uFFFD", "", tweet_clean$text, fixed =  TRUE)
tweet_clean$text <- gsub("\n", "", tweet_clean$text, fixed =  TRUE)
#Alfabetik olmayan karakterlerin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[^[:alnum:]]"," " )
steamCorpus <- Corpus(VectorSource(steam.df$text))
inspect(twitterCorpus[1:10])
steamCorpus <- tm_map(steamCorpus, content_transformer(tolower))
steamCorpus <- tm_map(steamCorpus, removeWords, stopwords("en"))
steamCorpus <- tm_map(steamCorpus, removeNumbers)
steamCorpus <- tm_map(steamCorpus, removePunctuation)
steamCorpus <- tm_map(steamCorpus, removere)
library(readxl)
turkish_stopwords <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/Turkish-Stopwords.xlsx")
turkish_stopwords
steamCorpus <- tm_map(steamCorpus, removeWords, turkish_stopwords)
durakkelimeler <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/durakkelimeler.xlsx")
durakkelimeler
steamCorpus <- tm_map(steamCorpus, removeWords, durakkelimeler)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeURL))
removeURL <- function(x) gsub("edua[[:alnum:]]*", "", x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeURL))
removeNonAscii <- function(x) textclean::replace_non_ascii(x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeNonAscii))
steamCorpus <- tm_map(steamCorpus, removeWords, c("amp", "ufef","ufeft", "uufefuufefuufef", "uufef", "s"))
steamCorpus <- tm_map(steamCorpus, stripWhitespace)
inspect(steamCorpus[1:10])
write.xlsx(steam.df, "C:/Users/samet/Desktop/twitter_steam_text_mining/25.12.2021tweets.xlsx")
strip_retweets(tweets, strip_manual = TRUE, strip_mt = TRUE)
strip_retweets(steamCorpus, strip_manual = TRUE, strip_mt = TRUE)
strip_retweets(steam$text, strip_manual = TRUE, strip_mt = TRUE)
strip_retweets(steam.df$text, strip_manual = TRUE, strip_mt = TRUE)
tweets = searchTwitter("stuff")
no_retweets = strip_retweets(tweets)
inspect(steamCorpus[1:10])
tweets = searchTwitter("steam")
no_retweets = strip_retweets(tweets)
inspect(steamCorpus[1:10])
steamCorpus = searchTwitter("steam")
no_retweets = strip_retweets(steamCorpus)
inspect(steamCorpus[1:10])
inspect(steamCorpus[1:10])
source("C:/Users/samet/Desktop/twitter_steam_text_mining/twitter_steam_text_mining.R")
library(ROAuth)
library(twitteR)
library(tm)
library(dplyr)
library(plyr)
library(purrr)
library(RCurl)
library(SnowballC)
library(openssl)
library(wordcloud)
library(ggplot2)
library(RColorBrewer)
library(stringr)
library(tidytext)
library(readr)
library(stringi)
library(hms)
library(lubridate)
library(igraph)
library(glue)
library(networkD3)
library(rtweet)
library(ggeasy)
library(plotly)
library(hms)
library(lubridate)
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
library(xlsx)
options(httr_oauth_cache=T)
APIkey <- "X4Ldvspy9rJPbQQvIgKZUJTD3"
APIsecret <- "e9RTNSQnOZN7G7cO0BJhQ3dFHNuV6cEF879ihjpcIeHStzEGXb"
access_token <- "802721334-yCrIMy3wIWFZyqmG8Qv8xSOQzgLlAp1qptXd4Sxa"
access_secret <- "DW3h24r4jHhASzSqwOvMSmqriFlCRoACWmKeLvlp4AFxQ"
setup_twitter_oauth(APIkey, APIsecret, access_token, access_secret)
steam <- searchTwitteR("steam", n=5000,  lang = "tr")
steam.df <- twListToDF(steam)
length(steam)
tweet_clean <- steam.df
tweet_clean$text <- stri_enc_toutf8(tweet_clean$text)
tweet_clean$text <- ifelse(str_sub(tweet_clean$text,1,2) == "RT",
substring(tweet_clean$text,3),
tweet_clean$text)
#URL linklerinin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "http[^[:space:]]*", "")
#Hashtag "#" ve "@" iÅaretlerinin kaldÄ±rÄ±lmasÄ±
tweet_clean$text <- str_replace_all(tweet_clean$text, "#\\S+", "")
tweet_clean$text <- str_replace_all(tweet_clean$text, "@\\S+", "")
#Noktalama iÅaretlerinin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[[:punct:][:blank:]]+", " ")
#TÃ¼m harflerin kÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅtÃ¼rÃ¼lmesi
tweet_clean$text  <- str_to_lower(tweet_clean$text, "tr")
#RakamlarÄ±n temizlenmesi
tweet_clean$text <- removeNumbers(tweet_clean$text)
#ASCII formatÄ±na uymayan karakterlerin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[<].*[>]", "")
tweet_clean$text <- gsub("\uFFFD", "", tweet_clean$text, fixed =  TRUE)
tweet_clean$text <- gsub("\n", "", tweet_clean$text, fixed =  TRUE)
#Alfabetik olmayan karakterlerin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[^[:alnum:]]"," " )
steamCorpus <- Corpus(VectorSource(steam.df$text))
inspect(steamCorpus[1:10])
steamCorpus <- tm_map(steamCorpus, content_transformer(tolower))
steamCorpus <- tm_map(steamCorpus, removeWords, stopwords("en"))
steamCorpus <- tm_map(steamCorpus, removeNumbers)
steamCorpus <- tm_map(steamCorpus, removePunctuation)
library(readxl)
turkish_stopwords <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/Turkish-Stopwords.xlsx")
turkish_stopwords
steamCorpus <- tm_map(steamCorpus, removeWords, turkish_stopwords)
durakkelimeler <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/durakkelimeler.xlsx")
durakkelimeler
steamCorpus <- tm_map(steamCorpus, removeWords, durakkelimeler)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeURL))
removeURL <- function(x) gsub("edua[[:alnum:]]*", "", x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeURL))
removeNonAscii <- function(x) textclean::replace_non_ascii(x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeNonAscii))
steamCorpus <- tm_map(steamCorpus, removeWords, c("amp", "ufef","ufeft", "uufefuufefuufef", "uufef", "s"))
steamCorpus <- tm_map(steamCorpus, stripWhitespace)
inspect(steamCorpus[1:10])
tidy_tweets <- tweet_clean %>% select(text) %>%
mutate(linenumber = row_number()) %>% unnest_tokens(word, text)
tidy_tweets <- tidy_tweets %>% anti_join(turkish_stopwords, by=c("word"="STOPWORD"))
tidy_tweets %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
tidy_tweets %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tidy_tweets <- tweet_clean %>% select(text) %>%
mutate(linenumber = row_number()) %>% unnest_tokens(word, text)
tidy_tweets <- tidy_tweets %>% anti_join(turkish_stopwords, by=c("word"="STOPWORD"))
tidy_tweets %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
tweet_clean %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
tidy_tweets <- tweet_clean %>% select(text) %>%
mutate(linenumber = row_number()) %>% unnest_tokens(word, text)
tidy_tweets <- tidy_tweets %>% anti_join(turkish_stopwords, by=c("word"="STOPWORD"))
tidy_tweets %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
tidy_tweets %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tweet_clean <- steam.df
tweet_clean$text <- stri_enc_toutf8(tweet_clean$text)
tweet_clean$text <- ifelse(str_sub(tweet_clean$text,1,2) == "RT",
substring(tweet_clean$text,3),
tweet_clean$text)
#URL linklerinin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "http[^[:space:]]*", "")
#Hashtag "#" ve "@" iÅaretlerinin kaldÄ±rÄ±lmasÄ±
tweet_clean$text <- str_replace_all(tweet_clean$text, "#\\S+", "")
tweet_clean$text <- str_replace_all(tweet_clean$text, "@\\S+", "")
#Noktalama iÅaretlerinin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[[:punct:][:blank:]]+", " ")
#TÃ¼m harflerin kÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅtÃ¼rÃ¼lmesi
tweet_clean$text  <- str_to_lower(tweet_clean$text, "tr")
#RakamlarÄ±n temizlenmesi
tweet_clean$text <- removeNumbers(tweet_clean$text)
#ASCII formatÄ±na uymayan karakterlerin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[<].*[>]", "")
tweet_clean$text <- gsub("\uFFFD", "", tweet_clean$text, fixed =  TRUE)
tweet_clean$text <- gsub("\n", "", tweet_clean$text, fixed =  TRUE)
#Alfabetik olmayan karakterlerin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[^[:alnum:]]"," " )
steamCorpus <- Corpus(VectorSource(steam.df$text))
inspect(steamCorpus[1:10])
steamCorpus <- tm_map(steamCorpus, content_transformer(tolower))
steamCorpus <- tm_map(steamCorpus, removeWords, stopwords("en"))
steamCorpus <- tm_map(steamCorpus, removeNumbers)
steamCorpus <- tm_map(steamCorpus, removePunctuation)
library(readxl)
turkish_stopwords <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/Turkish-Stopwords.xlsx")
turkish_stopwords
steamCorpus <- tm_map(steamCorpus, removeWords, turkish_stopwords)
durakkelimeler <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/durakkelimeler.xlsx")
durakkelimeler
steamCorpus <- tm_map(steamCorpus, removeWords, durakkelimeler)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeURL))
removeURL <- function(x) gsub("edua[[:alnum:]]*", "", x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeURL))
removeNonAscii <- function(x) textclean::replace_non_ascii(x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeNonAscii))
steamCorpus <- tm_map(steamCorpus, removeWords, c("amp", "ufef","ufeft", "uufefuufefuufef", "uufef", "s"))
steamCorpus <- tm_map(steamCorpus, stripWhitespace)
inspect(steamCorpus[1:10])
tidy_tweets <- tweet_clean %>% select(text) %>%
mutate(linenumber = row_number()) %>% unnest_tokens(word, text)
tidy_tweets <- tidy_tweets %>% anti_join(turkish_stopwords, by=c("word"="STOPWORD"))
tidy_tweets %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
tidy_tweets %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tidy_tweets %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
tidy_tweets %>%
count(word, sort = TRUE) %>%
filter(n > 5) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
library(ROAuth)
library(twitteR)
library(tm)
library(plyr)
library(dplyr)
library(purrr)
library(RCurl)
library(SnowballC)
library(openssl)
library(wordcloud)
library(ggplot2)
library(RColorBrewer)
library(stringr)
library(tidytext)
library(readr)
library(stringi)
library(hms)
library(lubridate)
library(igraph)
library(glue)
library(networkD3)
library(rtweet)
library(ggeasy)
library(plotly)
library(hms)
library(lubridate)
library(magrittr)
library(tidyverse)
library(janeaustenr)
library(widyr)
library(xlsx)
library(readxl)
steam <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/butun-tweetler.xlsx")
steam.df <- as.data.frame(steam)
tweet_clean <- steam.df
tweet_clean$text <- stri_enc_toutf8(tweet_clean$text)
tweet_clean$text <- ifelse(str_sub(tweet_clean$text,1,2) == "RT",
substring(tweet_clean$text,3),
tweet_clean$text)
#URL linklerinin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "http[^[:space:]]*", "")
tweet_clean$text <- str_replace_all(tweet_clean$text, "RT", "")
#Hashtag "#" ve "@" iÃÂaretlerinin kaldÃÂ±rÃÂ±lmasÃÂ±
tweet_clean$text <- str_replace_all(tweet_clean$text, "#\\S+", "")
tweet_clean$text <- str_replace_all(tweet_clean$text, "@\\S+", "")
#Noktalama iÃÂaretlerinin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[[:punct:][:blank:]]+", " ")
#TÃÂ¼m harflerin kÃÂ¼ÃÂ§ÃÂ¼k harfe dÃÂ¶nÃÂ¼ÃÂtÃÂ¼rÃÂ¼lmesi
tweet_clean$text  <- str_to_lower(tweet_clean$text, "tr")
#RakamlarÃÂ±n temizlenmesi
tweet_clean$text <- removeNumbers(tweet_clean$text)
#ASCII formatÃÂ±na uymayan karakterlerin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[<].*[>]", "")
tweet_clean$text <- gsub("\uFFFD", "", tweet_clean$text, fixed =  TRUE)
tweet_clean$text <- gsub("\n", "", tweet_clean$text, fixed =  TRUE)
#Alfabetik olmayan karakterlerin temizlenmesi
tweet_clean$text <- str_replace_all(tweet_clean$text, "[^[:alnum:]]"," " )
steamCorpus <- Corpus(VectorSource(steam.df$text))
inspect(steamCorpus[1:10])
steamCorpus <- tm_map(steamCorpus, content_transformer(tolower))
#steamCorpus <- tm_map(steamCorpus, removeWords, stopwords("en"))
steamCorpus <- tm_map(steamCorpus, removeNumbers)
steamCorpus <- tm_map(steamCorpus, removePunctuation)
turkish_stopwords <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/Turkish-Stopwords.xlsx")
turkish_stopwords
steamCorpus <- tm_map(steamCorpus, removeWords, turkish_stopwords)
durakkelimeler <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/durakkelimeler.xlsx")
durakkelimeler
steamCorpus <- tm_map(steamCorpus, removeWords, durakkelimeler)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeURL))
removeURL <- function(x) gsub("edua[[:alnum:]]*", "", x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeURL))
removeNonAscii <- function(x) textclean::replace_non_ascii(x)
steamCorpus <- tm_map(steamCorpus, content_transformer(removeNonAscii))
steamCorpus <- tm_map(steamCorpus, removeWords, c("amp", "ufef","ufeft", "uufefuufefuufef", "uufef", "s"))
steamCorpus <- tm_map(steamCorpus, stripWhitespace)
inspect(steamCorpus[1:10])
tidy_tweets <- tweet_clean %>% select(text) %>%
mutate(linenumber = row_number()) %>% unnest_tokens(word, text)
tidy_tweets <- tidy_tweets %>% anti_join(turkish_stopwords, by=c("word"="STOPWORD"))
tidy_tweets %>%
count(word, sort = TRUE) %>%
filter(n > 50) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
tidy_tweets %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
View(tweet_clean)
turkish_stopwords <- read_excel("C:/Users/samet/Desktop/twitter_steam_text_mining/Turkish-Stopwords.xlsx")
turkish_stopwords
steamCorpus <- tm_map(steamCorpus, removeWords, turkish_stopwords)
tidy_tweets <- tweet_clean %>% select(text) %>%
mutate(linenumber = row_number()) %>% unnest_tokens(word, text)
tidy_tweets <- tidy_tweets %>% anti_join(turkish_stopwords, by=c("word"="STOPWORD"))
tidy_tweets %>%
count(word, sort = TRUE) %>%
filter(n > 50) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
tidy_tweets %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
tidy_tweets %>%
count(word, sort = TRUE) %>%
filter(n > 50) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip() + theme_minimal() +
ggtitle("tweetlerde en cok kullanilan kelimeler")
View(tweet_clean)
pos.words <- scan('C:/Users/samet/Desktop/twitter_steam_text_mining/pozitifkelimeler.txt', what = 'character', comment.char = ';', skipNul = TRUE)
neg.words <- scan('C:/Users/samet/Desktop/twitter_steam_text_mining/negatifkelimeler.txt', what = 'character', comment.char = ';', skipNul = TRUE)
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
scores <- laply(sentences, function(sentence, pos.words, neg.words)
{
# clean up sentences with R's regex-driven global substitute, gsub() function:
sentence <- gsub('https://','',sentence)
sentence <- gsub('http://','',sentence)
sentence <- gsub('[^[:graph:]]', ' ',sentence)
sentence <- gsub('[[:punct:]]', '', sentence)
sentence <- gsub('[[:cntrl:]]', '', sentence)
sentence <- gsub('\\d+', '', sentence)
sentence <- str_replace_all(sentence,"[^[:graph:]]", " ")
# and convert to lower case:
sentence <- tolower(sentence)
# split into words. str_split is in the stringr package
word.list <- str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words <- unlist(word.list)
# compare our words to the dictionaries of positive & negative terms
pos.matches <- match(words, pos.words)
neg.matches <- match(words, neg.words)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
# TRUE/FALSE will be treated as 1/0 by sum():
score <- sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df <- data.frame(score=scores, text=sentences)
return(scores.df)
}
analysis <- score.sentiment(tweet_clean$text, pos.words, neg.words)
# sentiment score frequency table
table(analysis$score)
tweet_clean$text <- stri_enc_toutf8(tweet_clean$text)
analysis <- score.sentiment(tweet_clean$text, pos.words, neg.words)
# sentiment score frequency table
table(analysis$score)
analysis %>%
ggplot(aes(x=score)) +
geom_histogram(binwidth = 1, fill = "lightblue")+
ylab("Frequency") +
xlab("sentiment score") +
ggtitle("Distribution of Sentiment scores of the tweets") +
ggeasy::easy_center_title()
neutral <- length(which(analysis$score == 0))
positive <- length(which(analysis$score > 0))
negative <- length(which(analysis$score < 0))
Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
geom_bar(stat = "identity", aes(fill = Sentiment))+
ggtitle("Barplot of Sentiment type of 7500 tweets")
